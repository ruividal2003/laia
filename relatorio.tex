\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fancyhdr}

% Page setup
\geometry{margin=1in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\rhead{NYC Taxi Trip Duration Prediction}
\lhead{Technical Report}
\rfoot{Page \thepage}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
}

% Title information
\title{\textbf{Technical Report: \\NYC Taxi Trip Duration Prediction System}}
\author{LAIA Project Team}
\date{December 11, 2025}

\begin{document}

\maketitle
\thispagestyle{empty}

\newpage
\tableofcontents
\newpage


\section{Dataset and Preprocessing}
\subsection{Dataset Description}

The project utilizes the NYC Taxi and Limousine Commission (TLC) Trip Record Data, a publicly available dataset containing detailed information about taxi trips in New York City. The dataset spans multiple years with the following temporal distribution:

\begin{itemize}
    \item \textbf{Training Period}: 2011-2012
    \item \textbf{Testing Period}: 2013
\end{itemize}

The raw data is stored in Parquet format for efficient columnar storage and retrieval. Each trip record contains the following key attributes:

\begin{table}[H]
\centering
\caption{Dataset Attributes}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Attribute} & \textbf{Description} \\ \midrule
Pickup DateTime & Timestamp of trip initiation \\
Dropoff DateTime & Timestamp of trip completion \\
Trip Distance & Distance traveled in miles \\
Passenger Count & Number of passengers \\
Pickup Coordinates & Latitude and longitude of pickup location \\
Dropoff Coordinates & Latitude and longitude of dropoff location \\
Vendor ID & Taxi vendor identifier \\
Location IDs & Pickup and dropoff zone identifiers \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Memory-Efficient Data Loading}

The dataset contains millions of taxi trip records, which can exceed available system memory. To address this challenge, the system implements a controlled loading strategy that reads data in manageable portions.

The data loader reads multiple Parquet files and applies sampling to limit the total number of records processed. This allows the model to train on representative data without requiring excessive computational resources.

The system enforces the following limits:
\begin{itemize}
    \item Training data: Maximum 20 million rows
    \item Validation data: Maximum 10 million rows
\end{itemize}

For testing and development environments where actual taxi data may not be available, the system can generate synthetic trip records to ensure the training pipeline remains functional.

\subsection{Data Preprocessing Pipeline}

Before training the model, raw trip records must be cleaned and transformed into a format suitable for machine learning. The preprocessing pipeline performs several essential operations to ensure data quality and extract meaningful patterns.

\textbf{Column Normalization}: The first step standardizes column names across different file versions. Since the TLC dataset format has changed over the years, different files may use different column names for the same information (e.g., \texttt{tpep\_pickup\_datetime} vs \texttt{pickup\_datetime}). The pipeline renames columns to a consistent standard.

\subsubsection{Target Variable Construction}

\textbf{Calculating Trip Duration}: The model's goal is to predict how long a taxi trip will take. Trip duration is calculated by subtracting the pickup time from the dropoff time, measured in minutes.

\textbf{Data Quality Filtering}: To ensure reliable training data, the pipeline removes problematic records:
\begin{itemize}
    \item Trips with duration $\leq$ 0 minutes (indicates recording errors)
    \item Trips exceeding 180 minutes (3 hours), which are statistical outliers
\end{itemize}

This filtering step removes approximately 1-2\% of records that likely contain GPS errors, incorrect timestamps, or represent exceptional circumstances that would confuse the model.

\subsubsection{Temporal Feature Engineering}

\textbf{Extracting Time-Based Patterns}: Taxi trip durations vary significantly depending on when the trip occurs. To capture these patterns, the pipeline extracts three time-based features from the pickup timestamp:

\begin{itemize}
    \item \textbf{Pickup Hour} (0-23): Captures daily patterns such as rush hours (7-9 AM, 5-7 PM) when traffic is heavy and trips take longer
    \item \textbf{Day of Week} (0-6): Distinguishes weekdays (typically more traffic) from weekends (lighter traffic, different trip patterns)
    \item \textbf{Month} (1-12): Captures seasonal effects like tourist season, holidays, or weather patterns
\end{itemize}

By providing these time indicators, the model can learn that, for example, a 5-mile trip at 8 AM on Monday typically takes longer than the same trip at 2 PM on Sunday.

\subsubsection{Distance Feature Processing}

\textbf{Unit Conversion}: The original dataset records trip distance in miles. The preprocessing pipeline converts this to kilometers using the conversion factor 1.60934 km/mile. This conversion creates the \texttt{hav\_km} feature.

Both the original distance (miles) and converted distance (kilometers) are retained as features, providing the model with consistent distance information. This dual representation ensures that the model receives distance information in both measurement systems, which can improve prediction accuracy.

\subsubsection{Data Quality Assurance}
The final preprocessing stage ensures data integrity:
\begin{itemize}
    \item Validates presence of required columns
    \item Imputes missing passenger counts with a default value of 1
    \item Removes records with null values in critical features
    \item Verifies data types and value ranges
\end{itemize}

\subsection{Final Feature Set}

After preprocessing, each trip record is represented by six features:

\begin{enumerate}
    \item \textbf{trip\_distance}: Original distance in miles
    \item \textbf{hav\_km}: Distance in kilometers
    \item \textbf{passenger\_count}: Number of passengers (1-8 typical range)
    \item \textbf{pickup\_hour}: Hour of day (0-23)
    \item \textbf{pickup\_dow}: Day of week (0-6)
    \item \textbf{pickup\_mon}: Month (1-12)
\end{enumerate}

The target variable \textbf{duration\_min} represents the trip duration in minutes that the model aims to predict.

\section{Model Architecture and Training Strategy}

\subsection{Algorithm Selection}

The system employs Ridge Regression, a linear model with L2 regularization, as the prediction algorithm. This choice is motivated by several factors:

\begin{itemize}
    \item \textbf{Interpretability}: Linear models provide transparent feature-target relationships
    \item \textbf{Computational Efficiency}: Training and inference scale linearly with dataset size
    \item \textbf{Regularization}: L2 penalty prevents overfitting on correlated features
    \item \textbf{Baseline Performance}: Establishes a performance floor for future model iterations
\end{itemize}

Ridge regression solves the optimization problem:

\begin{equation}
\min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 + \alpha \|\beta\|_2^2 \right\}
\end{equation}

where $\alpha$ is the regularization parameter, set to 1.0 in this implementation.

\subsection{Model Pipeline Architecture}

The model is implemented as a scikit-learn Pipeline combining preprocessing and prediction stages:

\begin{enumerate}
    \item \textbf{Feature Scaling}: StandardScaler normalizes all numerical features to zero mean and unit variance
    \item \textbf{Regression Model}: Ridge regression with $\alpha = 1.0$
\end{enumerate}

The StandardScaler transformation is critical for Ridge regression, as L2 regularization is scale-sensitive. Without normalization, features with larger magnitudes would be disproportionately penalized.

The scaling transformation for feature $x_j$ is:

\begin{equation}
z_j = \frac{x_j - \mu_j}{\sigma_j}
\end{equation}

where $\mu_j$ and $\sigma_j$ are the mean and standard deviation computed on the training set.

\subsection{Training Methodology}
\subsubsection{Data Splitting Strategy}

The evaluation strategy uses two complementary approaches:

\textbf{Cross-Validation (Internal Validation)}: The 2011-2012 data is used for 5-fold cross-validation. This provides stable performance estimates during model development.

\textbf{Temporal Validation (External Validation)}: After training on all 2011-2012 data, the model is evaluated on 2013 data. This tests whether the model can predict trip durations in a future time period it has never seen, which is the realistic deployment scenario.

This dual validation approach ensures the model is both internally consistent and capable of generalizing to future data.

\subsubsection{Training Process}

The training process follows a systematic approach to ensure the model performs reliably:

\textbf{Cross-Validation Strategy}: Rather than splitting the data into just training and validation sets, the system employs 5-fold cross-validation. This technique divides the training data into 5 parts, trains the model 5 times (each time using a different part for validation), and averages the results. This provides a more reliable estimate of model performance and reduces the risk of overfitting to a particular data split.

\textbf{Training Workflow}:
\begin{enumerate}
    \item \textbf{Data Loading}: Load taxi trip records from 2011-2012 (training period)
    \item \textbf{Preprocessing}: Apply all cleaning and feature engineering steps
    \item \textbf{Cross-Validation}: Train and evaluate model across 5 different data splits
    \item \textbf{Full Training}: Train final model on all 2011-2012 data
    \item \textbf{Temporal Validation}: Evaluate on separate 2013 data to test generalization to future time periods
    \item \textbf{Model Saving}: Serialize the trained model for deployment
\end{enumerate}

\subsection{Evaluation Metrics}

Model performance is quantified using two complementary metrics:

\textbf{Mean Absolute Error (MAE)}:
\begin{equation}
\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|
\end{equation}

MAE provides an interpretable measure of average prediction error in minutes. It is robust to outliers and directly represents the typical deviation between predicted and actual trip durations.

\textbf{Root Mean Squared Error (RMSE)}:
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
\end{equation}

RMSE penalizes larger errors more heavily than MAE, providing insight into prediction variance. A large RMSE relative to MAE indicates the presence of significant outliers in predictions.
Both metrics are computed on the Validation set, and Test set.

\subsection{Hyperparameter Configuration}
The model configuration employs the following hyperparameters:

\begin{table}[H]
\centering
\caption{Model Hyperparameters}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Rationale} \\ \midrule
Ridge $\alpha$ & 1.0 & Standard regularization strength \\
Random State & 42 & Ensures reproducibility \\
Max Train Rows & 20,000,000 & Memory constraint balance \\
Max Test Rows & 10,000,000 & Evaluation efficiency \\
Test Size & 0.2 & Standard 80/20 split \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Model Persistence}
The trained pipeline is serialized using two mechanisms:
\begin{enumerate}
    \item \textbf{MLflow Artifact Storage}: Maintains version-controlled model registry
    \item \textbf{Joblib Serialization}: Creates deployment-ready binary artifact
\end{enumerate}

\section{MLOps Pipeline Design and Implementation}

\subsection{Pipeline Architecture Overview}
The MLOps infrastructure implements a complete machine learning lifecycle encompassing training, experimentation, deployment, and monitoring. The architecture follows industry best practices for reproducibility, scalability, and maintainability.

The pipeline consists of three primary components:
\begin{enumerate}
    \item \textbf{Training Pipeline}: Data loading, preprocessing, model training, and evaluation
    \item \textbf{Experiment Tracking}: MLflow server for versioning and metrics logging
    \item \textbf{Inference Service}: FastAPI-based REST API for real-time predictions
\end{enumerate}

\subsection{Experiment Tracking with MLflow}

MLflow provides comprehensive experiment management capabilities, tracking all aspects of model development.

\subsubsection{MLflow Architecture}

The MLflow server operates as a containerized service with the following configuration:
\begin{itemize}
    \item \textbf{Backend Store}: SQLite database stores experiment metadata (parameters, metrics, timestamps)
    \item \textbf{Artifact Store}: File system stores model binaries and training artifacts
    \item \textbf{Port}: 5000 (accessible via web browser for experiment visualization and via API for programmatic access)
    \item \textbf{Persistence}: Docker volume mount ensures data survives container restarts
\end{itemize}

\subsubsection{Tracked Information}

Each training run logs comprehensive information to MLflow:

\textbf{Parameters}:
\begin{itemize}
    \item Model type and algorithm
    \item Hyperparameter values (regularization strength)
    \item Data configuration (file patterns, row limits)
\end{itemize}

\textbf{Metrics}:
\begin{itemize}
    \item Validation MAE and RMSE
    \item Test MAE and RMSE
    \item Training duration
\end{itemize}

\textbf{Artifacts}:
\begin{itemize}
    \item Serialized model pipeline
    \item Feature preprocessing parameters
    \item Metadata and configuration files
\end{itemize}

This logging enables comparison across experiments, facilitating model selection and hyperparameter optimization.

\subsubsection{Benefits of MLflow Integration}

The experiment tracking system provides several operational advantages:
\begin{itemize}
    \item \textbf{Version Control}: Every model version is tracked with associated metrics
    \item \textbf{Reproducibility}: Complete parameter and artifact history
    \item \textbf{Comparison}: Side-by-side evaluation of multiple experiments
    \item \textbf{Model Registry}: Centralized repository for production models
    \item \textbf{Audit Trail}: Complete history of model development
\end{itemize}


\subsection{Containerization Strategy}

The inference service is containerized using Docker, ensuring consistent deployment across environments.

\subsubsection{Container Architecture}

The Docker image is built on a minimal Python 3.10 base image with the following layers:
\begin{enumerate}
    \item \textbf{Base Layer}: Python 3.10 slim distribution (minimal system dependencies)
    \item \textbf{Dependency Layer}: Python packages from requirements specification
    \item \textbf{Application Layer}: Source code and model artifacts
    \item \textbf{Runtime Configuration}: Environment variables and entry point
\end{enumerate}

This layered approach optimizes build cache utilization and minimizes image size.


\subsubsection{Service Deployment}

The containerized API runs under Uvicorn, a high-performance ASGI server optimized for async Python applications. The server configuration exposes the service on port 8080 and binds to all network interfaces for accessibility. The API provides two main endpoints:

\textbf{Health Check Endpoint}:
\begin{itemize}
    \item \textbf{Route}: GET /health
    \item \textbf{Purpose}: Service availability verification
    \item \textbf{Response}: Status indicator (\texttt{\{"status": "ok"\}})
\end{itemize}

\textbf{Prediction Endpoint}:
\begin{itemize}
    \item \textbf{Route}: POST /predict
    \item \textbf{Input}: JSON array of trip records with vendor information, timestamps, passenger count, trip distance, and location identifiers
    \item \textbf{Output}: JSON array of predicted durations in minutes
    \item \textbf{Features}: Batch prediction support for multiple trips simultaneously
\end{itemize}

The API implements lazy model loading: the trained model file is not loaded into memory when the service starts. Instead, it is loaded only when the first prediction request arrives. This design choice offers two advantages: (1) the service starts faster, and (2) if the service is running but receives no requests, it uses less memory. This is particularly useful in development and testing environments.

\subsection{Orchestration with Docker Compose}

The system uses Docker Compose for multi-service orchestration, currently managing the MLflow tracking server with extensibility for additional services.

\subsection{Configuration Management}
The system employs an environment-based configuration for operational flexibility:

\begin{table}[H]
\centering
\caption{System Configuration Parameters}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter} & \textbf{Default} & \textbf{Purpose} \\ \midrule
BASE\_URL & cloudfront.net/trip-data & Data source URL \\
TRAIN\_DATA & 2011,2012 & Training years \\
VALIDATION\_DATA & 2013 & Validation year \\
MAX\_TRAIN\_ROWS & 20,000,000 & Training data limit \\
MAX\_TEST\_ROWS & 10,000,000 & Test data limit \\
MLFLOW\_EXPERIMENT\_NAME & nyc\_taxi\_baseline & Experiment identifier \\
MLFLOW\_MODEL\_NAME & nyc\_taxi\_duration\_model & Model registry name \\
MODEL\_PATH & artifacts/model.pkl & Local model file \\
MLFLOW\_TRACKING\_URI & http://localhost:5000 & MLflow server address \\ \bottomrule
\end{tabular}
\end{table}

This configuration strategy enables environment-specific settings without code modification.

\subsection{Testing Strategy}

The system implements a multi-level testing approach:

\subsubsection{Unit Tests}

Unit tests validate individual components:
\begin{itemize}
    \item Feature engineering correctness (temporal feature extraction, distance conversion)
    \item Data preprocessing logic (outlier removal, null handling)
    \item API endpoint functionality (health checks, prediction schema)
    \item Model pipeline construction
\end{itemize}

\subsubsection{Integration Tests}

Integration tests verify end-to-end workflows:
\begin{itemize}
    \item Complete training pipeline execution
    \item API prediction request processing
    \item MLflow logging functionality
\end{itemize}

\subsubsection{Validation Scripts}

A dedicated validation script performs offline evaluation on real trip data, comparing predicted versus actual durations to verify model sanity before deployment.

\subsection{Deployment Workflow}

\begin{enumerate}
    \item \textbf{Model Training}: Execute training script to fit model on latest data
    \item \textbf{Evaluation}: Review metrics logged to MLflow for performance assessment
    \item \textbf{Validation}: Run validation script on held-out test data
    \item \textbf{Artifact Export}: Serialize model to deployment artifact location
    \item \textbf{Container Build}: Package application and model into Docker image
    \item \textbf{Service Launch}: Deploy container with appropriate configuration
    \item \textbf{Health Verification}: Confirm service availability via health endpoint
    \item \textbf{Smoke Testing}: Execute sample predictions to verify correctness
\end{enumerate}

This workflow ensures validation before production deployment.


\subsubsection{Continuous Integration/Continuous Deployment}
Automated pipelines for testing and deployment triggered by code changes or model updates, ensuring rapid iteration cycles.

\subsection{Data Pipeline Architecture}

The complete data flow through the system follows this sequence:

\begin{enumerate}
    \item Raw Parquet files stored in filesystem
    \item Memory-efficient loading with row sampling
    \item Preprocessing and feature engineering
    \item Model training on processed features
    \item Model serialization and artifact storage
    \item API service loads model for inference
    \item Incoming prediction requests transformed via identical preprocessing
    \item Model generates predictions returned via API response
\end{enumerate}

\section{Results and Performance}

\subsection{Model Performance Evaluation}

The Ridge regression model was evaluated using a comprehensive validation strategy that includes both cross-validation on the training data and temporal out-of-sample testing on future data.

\subsubsection{Cross-Validation Results}

The 5-fold cross-validation on the 2011-2012 training data provides robust estimates of model performance:

\begin{itemize}
    \item \textbf{CV MAE Mean}: 
    \item \textbf{CV MAE Standard Deviation}:
    \item \textbf{CV RMSE Mean}: 
    \item \textbf{CV RMSE Standard Deviation}: 
\end{itemize}

The cross-validation approach ensures that the model's performance is not dependent on a particular data split and provides confidence intervals for the expected prediction accuracy.

\subsubsection{Validation Set Performance}


\subsubsection{Performance Analysis}
The model demonstrates good generalization capability. When comparing the cross-validation results (on 2011-2012 data) with the temporal validation results (on 2013 data), the performance metrics remain similar. This indicates that the model successfully learns general patterns rather than memorizing specific characteristics of the training data.

The Ridge regression's regularization parameter (L2 penalty) helps prevent overfitting by penalizing overly complex models. The consistent performance across different evaluation strategies confirms that this regularization is working effectively.

\subsection{Model Characteristics}

\subsubsection{Strengths}
\begin{itemize}
    \item \textbf{Interpretability}: Linear coefficients reveal feature importance and direction of influence
    \item \textbf{Computational Efficiency}: Fast training (minutes on millions of records) and sub-millisecond inference
    \item \textbf{Robustness}: Regularization prevents overfitting on collinear features (e.g., trip\_distance and hav\_km)
    \item \textbf{Temporal Stability}: Consistent performance across different time periods
    \item \textbf{Baseline Establishment}: Provides performance benchmark for more complex models
\end{itemize}

\subsubsection{Limitations}
\begin{itemize}
    \item \textbf{Linearity Assumption}: Cannot capture complex non-linear relationships between features (e.g., exponential traffic increase during rush hours)
    \item \textbf{Feature Limitations}: Lacks external factors such as weather conditions, special events, or real-time traffic information
    \item \textbf{Spatial Modeling}: Simplified distance features miss route-specific patterns and neighborhood-specific traffic behaviors
    \item \textbf{Temporal Resolution}: Fixed hourly features may miss sub-hourly traffic variations
\end{itemize}

\section{Conclusion \& Project Achievements}

This project successfully implements a complete end-to-end machine learning system for predicting NYC taxi trip durations, demonstrating proficiency in modern MLOps practices and data engineering.

\subsection{Achievements}
\subsubsection{Technical Implementation}
\begin{itemize}
    \item \textbf{Scalable Data Pipeline}: Implemented memory-efficient processing of large-scale taxi trip data (20+ million records) with configurable sampling strategies
    \item \textbf{Feature Engineering}: Developed systematic temporal and spatial feature transformations that capture time-dependent traffic patterns
    \item \textbf{Model Development}: Trained and evaluated Ridge regression model with cross-validation, achieving consistent performance across temporal validation sets
    \item \textbf{Experiment Tracking}: Integrated MLflow for comprehensive tracking of model parameters, metrics, and artifacts
    \item \textbf{Production API}: Deployed FastAPI-based REST service with health monitoring and batch prediction capabilities
    \item \textbf{Containerization}: Created Docker containers for both training and serving components, ensuring reproducibility
\end{itemize}

\subsubsection{MLOps Infrastructure}
\begin{itemize}
    \item \textbf{Continuous Integration}: Implemented automated testing pipelines that validate model training and API functionality
    \item \textbf{Continuous Deployment}: Established workflows for automated model training, evaluation, and deployment
    \item \textbf{Model Registry}: Utilized MLflow's model registry with aliasing for version control and deployment management
    \item \textbf{Configuration Management}: Developed environment-based configuration system supporting multiple deployment contexts
\end{itemize}

\subsubsection{Software Engineering Best Practices}
\begin{itemize}
    \item \textbf{Modular Architecture}: Separated training and serving components for independent development and deployment
    \item \textbf{Testing Framework}: Implemented unit and integration tests for critical components
    \item \textbf{Documentation}: Maintained clear documentation of configuration parameters and deployment procedures
    \item \textbf{Version Control}: Utilized Git with structured workflows for collaborative development
\end{itemize}

\subsection{What we Learned}
This project provided practical experience with:
\begin{itemize}
    \item Designing and implementing production machine learning systems
    \item Working with large-scale real-world datasets
    \item Applying MLOps principles for experiment tracking and model deployment
    \item Containerization and orchestration of ML services
    \item API design for machine learning inference
    \item Balancing model complexity with interpretability and computational efficiency
\end{itemize}

\subsection{Future Work}
\begin{itemize}
    \item Experiment with non-linear models (gradient boosting, neural networks) to capture complex traffic patterns
    \item Incorporate geospatial features using pickup/dropoff coordinates with techniques like geohashing or zone embeddings
    \item Integrate external data sources (weather APIs, event calendars, real-time traffic feeds)
    \item Implement ensemble methods combining multiple model predictions
\end{itemize}

\subsection{Conclusion}

The NYC Taxi Trip Duration Prediction system demonstrates that even relatively simple models, when properly engineered and deployed, can provide valuable predictions for real-world applications.
This project establishes a solid foundation for taxi trip duration prediction while maintaining simplicity and interpretability. The modular architecture and comprehensive documentation facilitate future enhancements, whether through more sophisticated modeling techniques or expanded data sources. Most importantly, the system exemplifies how machine learning engineering extends beyond model training to encompass the complete lifecycle of data processing, model development, deployment, and monitoring.


\section*{References}

\begin{enumerate}[label={[\arabic*]}]
    \item NYC Taxi and Limousine Commission. \textit{TLC Trip Record Data}. Available at: \url{https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page}
    
    \item Zaharia, M., et al. (2018). \textit{MLflow: A Platform for the Machine Learning Lifecycle}. Available at: \url{https://mlflow.org/}
    
    \item Ram√≠rez, S. (2021). \textit{FastAPI: Modern, fast web framework for building APIs with Python}. Available at: \url{https://fastapi.tiangolo.com/}
    
    \item Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. \textit{Journal of Machine Learning Research}, 12, 2825-2830.
    
    \item Hoerl, A. E., \& Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. \textit{Technometrics}, 12(1), 55-67.
    
    \item McKinney, W. (2010). Data Structures for Statistical Computing in Python. \textit{Proceedings of the 9th Python in Science Conference}, 56-61.
\end{enumerate}

\end{document}
